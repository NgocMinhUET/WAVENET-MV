#!/usr/bin/env python3
"""
Test script Ä‘á»ƒ kiá»ƒm tra cÃ¡c fix Ä‘Ã£ thá»±c hiá»‡n
"""

import os
import sys
import subprocess
from pathlib import Path

# Fix OpenMP warning
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"

def test_ms_ssim_fix():
    """Test MS-SSIM fix vá»›i áº£nh nhá»"""
    print("ğŸ§ª Testing MS-SSIM fix with small images...")
    
    try:
        import numpy as np
        from skimage.metrics import structural_similarity as ssim
        
        # Táº¡o áº£nh nhá» (5x5) Ä‘á»ƒ test
        img1 = np.random.rand(5, 5, 3)
        img2 = np.random.rand(5, 5, 3)
        
        # Test safe_ssim function
        def safe_ssim(im1, im2, data_range):
            try:
                H, W = im1.shape[:2] if im1.ndim >= 2 else im1.shape
                min_dim = min(H, W)
                if min_dim < 7:
                    if min_dim < 3:
                        return np.corrcoef(im1.flatten(), im2.flatten())[0, 1]
                    else:
                        win_size = min_dim if min_dim % 2 == 1 else min_dim - 1
                        return ssim(im1, im2, data_range=data_range, win_size=win_size)
                else:
                    return ssim(im1, im2, data_range=data_range)
            except Exception as e:
                return np.corrcoef(im1.flatten(), im2.flatten())[0, 1]
        
        # Test vá»›i áº£nh nhá»
        result = safe_ssim(img1, img2, 1.0)
        print(f"âœ… MS-SSIM with small image (5x5): {result:.4f}")
        
        # Test vá»›i áº£nh lá»›n hÆ¡n
        img1_large = np.random.rand(10, 10, 3)
        img2_large = np.random.rand(10, 10, 3)
        result_large = safe_ssim(img1_large, img2_large, 1.0)
        print(f"âœ… MS-SSIM with larger image (10x10): {result_large:.4f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ MS-SSIM test failed: {e}")
        return False

def test_script_arguments():
    """Test script arguments"""
    print("\nğŸ§ª Testing script arguments...")
    
    # Test evaluate_vcm.py
    try:
        result = subprocess.run([
            sys.executable, "evaluate_vcm.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… evaluate_vcm.py arguments OK")
        else:
            print(f"âŒ evaluate_vcm.py arguments failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ evaluate_vcm.py test failed: {e}")
        return False
    
    # Test codec_metrics_final.py
    try:
        result = subprocess.run([
            sys.executable, "evaluation/codec_metrics_final.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… codec_metrics_final.py arguments OK")
        else:
            print(f"âŒ codec_metrics_final.py arguments failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ codec_metrics_final.py test failed: {e}")
        return False
    
    # Test compare_baselines.py
    try:
        result = subprocess.run([
            sys.executable, "evaluation/compare_baselines.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… compare_baselines.py arguments OK")
        else:
            print(f"âŒ compare_baselines.py arguments failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ compare_baselines.py test failed: {e}")
        return False
    
    return True

def test_new_scripts():
    """Test cÃ¡c script má»›i Ä‘Æ°á»£c táº¡o"""
    print("\nğŸ§ª Testing new scripts...")
    
    scripts_to_test = [
        "evaluation/generate_paper_results.py",
        "evaluation/generate_tables.py", 
        "evaluation/statistical_analysis.py",
        "evaluation/generate_summary_report.py"
    ]
    
    for script in scripts_to_test:
        if os.path.exists(script):
            try:
                result = subprocess.run([
                    sys.executable, script, "--help"
                ], capture_output=True, text=True, timeout=10)
                
                if result.returncode == 0:
                    print(f"âœ… {script} OK")
                else:
                    print(f"âŒ {script} failed: {result.stderr}")
                    return False
                    
            except Exception as e:
                print(f"âŒ {script} test failed: {e}")
                return False
        else:
            print(f"âŒ {script} not found")
            return False
    
    return True

def test_run_complete_evaluation():
    """Test run_complete_evaluation.py vá»›i arguments Ä‘Ã£ sá»­a"""
    print("\nğŸ§ª Testing run_complete_evaluation.py...")
    
    try:
        result = subprocess.run([
            sys.executable, "run_complete_evaluation.py", "--help"
        ], capture_output=True, text=True, timeout=10)
        
        if result.returncode == 0:
            print("âœ… run_complete_evaluation.py arguments OK")
            
            # Kiá»ƒm tra xem cÃ³ sá»­ dá»¥ng Ä‘Ãºng arguments khÃ´ng
            help_text = result.stdout
            if "--output_json" in help_text and "--output_csv" in help_text:
                print("âœ… Correct arguments detected")
                return True
            else:
                print("âŒ Wrong arguments detected")
                return False
        else:
            print(f"âŒ run_complete_evaluation.py failed: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ run_complete_evaluation.py test failed: {e}")
        return False

def main():
    print("ğŸ”§ Testing WAVENET-MV Evaluation Fixes")
    print("=" * 50)
    
    tests = [
        ("MS-SSIM Fix", test_ms_ssim_fix),
        ("Script Arguments", test_script_arguments),
        ("New Scripts", test_new_scripts),
        ("Complete Evaluation", test_run_complete_evaluation)
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        print(f"\nğŸ“‹ Running {test_name}...")
        if test_func():
            print(f"âœ… {test_name} PASSED")
            passed += 1
        else:
            print(f"âŒ {test_name} FAILED")
    
    print(f"\nğŸ“Š Test Results: {passed}/{total} tests passed")
    
    if passed == total:
        print("ğŸ‰ All tests passed! Evaluation pipeline is ready.")
        print("\nğŸ“ Next steps:")
        print("1. Run on server: git pull origin master")
        print("2. Test with real data: python run_complete_evaluation.py --stage1_checkpoint ...")
        print("3. Check results in results/ directory")
    else:
        print("âš ï¸ Some tests failed. Please check the errors above.")

if __name__ == "__main__":
    main() 