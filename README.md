# WAVENET-MV: Wavelet-based Video Compression for Machine Vision

Framework nÃ©n video thÃ´ng minh káº¿t há»£p Wavelet Transform vÃ  Adaptive Mixing Network (AdaMixNet) Ä‘á»ƒ optimize cho cÃ¡c tÃ¡c vá»¥ AI trÃªn datasets COCO 2017 vÃ  DAVIS 2017.

## ğŸš€ TÃ­nh nÄƒng chÃ­nh

- **Wavelet Transform CNN**: Lifting-based wavelet decomposition vá»›i PredictCNN vÃ  UpdateCNN
- **AdaMixNet**: Adaptive mixing vá»›i 4 parallel filters vÃ  attention mechanism
- **Compressor VNVC**: Quantization vÃ  entropy coding vá»›i CompressAI GaussianConditional
- **AI Heads**: YOLO-tiny detection vÃ  SegFormer-lite segmentation trÃªn compressed features
- **3-Stage Training**: Pre-training â†’ Rate-Distortion â†’ Task-specific training
- **Multi-Lambda Support**: Î» âˆˆ {256, 512, 1024} cho different compression rates

## ğŸ“‹ Requirements

```bash
# CÃ i Ä‘áº·t dependencies
pip install -r requirements.txt
```

### Core Dependencies
- PyTorch â‰¥ 1.13.0
- TorchVision â‰¥ 0.14.0
- CompressAI â‰¥ 1.2.0
- OpenCV, NumPy, SciPy
- TensorBoard, Matplotlib, Seaborn
- COCO API, Albumentations

## ğŸ—ï¸ Cáº¥u trÃºc dá»± Ã¡n

```
WAVENET-MV/
â”œâ”€â”€ models/                          # Core models
â”‚   â”œâ”€â”€ wavelet_transform_cnn.py     # WaveletTransformCNN vá»›i PredictCNN/UpdateCNN
â”‚   â”œâ”€â”€ adamixnet.py                 # AdaMixNet vá»›i 4 parallel filters
â”‚   â”œâ”€â”€ compressor_vnvc.py           # Compressor vá»›i quantization + entropy
â”‚   â””â”€â”€ ai_heads.py                  # YOLO-tiny + SegFormer-lite heads
â”œâ”€â”€ training/                        # Training scripts
â”‚   â”œâ”€â”€ stage1_train_wavelet.py      # Stage 1: Wavelet pre-training
â”‚   â”œâ”€â”€ stage2_train_compressor.py   # Stage 2: Rate-distortion training
â”‚   â””â”€â”€ stage3_train_ai.py           # Stage 3: AI heads training
â”œâ”€â”€ evaluation/                      # Evaluation utilities
â”‚   â”œâ”€â”€ codec_metrics.py             # PSNR, MS-SSIM, BPP calculation
â”‚   â”œâ”€â”€ task_metrics.py              # mAP, IoU, Top-1 metrics
â”‚   â”œâ”€â”€ plot_rd_curves.py            # Rate-distortion curves
â”‚   â””â”€â”€ compare_baselines.py         # Baseline comparison
â”œâ”€â”€ datasets/                        # Dataset setup vÃ  loaders
â”‚   â”œâ”€â”€ setup_coco.sh                # COCO dataset setup script
â”‚   â”œâ”€â”€ setup_davis.sh               # DAVIS dataset setup script
â”‚   â””â”€â”€ dataset_loaders.py           # PyTorch dataset loaders
â”œâ”€â”€ checkpoints/                     # Model checkpoints
â”œâ”€â”€ runs/                            # TensorBoard logs
â”œâ”€â”€ fig/                             # Generated plots
â””â”€â”€ PROJECT_CONTEXT.md               # Context tracking cho development
```

## ğŸ”§ Dataset Setup

### COCO 2017
```bash
# CÃ i Ä‘áº·t COCO dataset
chmod +x datasets/setup_coco.sh
bash datasets/setup_coco.sh
```

### DAVIS 2017
```bash
# CÃ i Ä‘áº·t DAVIS dataset
chmod +x datasets/setup_davis.sh
bash datasets/setup_davis.sh
```

## ğŸ¯ Training Pipeline

### Stage 1: Wavelet Pre-training (30 epochs)
```bash
# Pre-train WaveletTransformCNN vá»›i L2 reconstruction loss
python training/stage1_train_wavelet.py \
    --dataset coco \
    --epochs 30 \
    --batch_size 8 \
    --learning_rate 2e-4 \
    --image_size 256
```

### Stage 2: Rate-Distortion Training (40 epochs)
```bash
# Train compressor vá»›i RD loss, freeze wavelet weights
python training/stage2_train_compressor.py \
    --dataset coco \
    --epochs 40 \
    --batch_size 8 \
    --lambda_rd 256 \
    --wavelet_checkpoint checkpoints/stage1_wavelet_coco_best.pth
```

### Stage 3: AI Heads Training (50 epochs)
```bash
# Train AI heads trÃªn compressed features, freeze compressor
python training/stage3_train_ai.py \
    --task detection \
    --dataset coco \
    --epochs 50 \
    --batch_size 8 \
    --model yolo-tiny \
    --compressor_checkpoint checkpoints/stage2_compressor_coco_best.pth

# Segmentation training
python training/stage3_train_ai.py \
    --task segmentation \
    --dataset davis \
    --epochs 50 \
    --batch_size 8 \
    --model segformer-lite \
    --compressor_checkpoint checkpoints/stage2_compressor_davis_best.pth
```

## ğŸ“Š Evaluation

### Codec Performance
```bash
# Evaluate compression metrics
python evaluation/codec_metrics.py \
    --dataset coco \
    --split val \
    --checkpoint checkpoints/stage2_compressor_coco_best.pth \
    --output_csv results/codec_metrics.csv
```

### Task Performance
```bash
# Object detection evaluation
python evaluation/task_metrics.py \
    --task detection \
    --dataset coco \
    --checkpoint checkpoints/stage3_detection_coco_best.pth \
    --output_csv results/detection_metrics.csv

# Video segmentation evaluation
python evaluation/task_metrics.py \
    --task segmentation \
    --dataset davis \
    --checkpoint checkpoints/stage3_segmentation_davis_best.pth \
    --output_csv results/segmentation_metrics.csv
```

### Rate-Distortion Curves
```bash
# Generate RD curves cho paper
python evaluation/plot_rd_curves.py \
    --checkpoints checkpoints/stage2_compressor_*_best.pth \
    --output fig/rd_curves.png \
    --lambdas 256 512 1024
```

### Baseline Comparison
```bash
# So sÃ¡nh vá»›i HEVC, VVC, vÃ  DVC
python evaluation/compare_baselines.py \
    --dataset coco \
    --wavenet_checkpoint checkpoints/stage3_detection_coco_best.pth \
    --output_csv results/baseline_comparison.csv
```

## ğŸ§ª Testing

### Unit Tests
```bash
# Test individual components
python models/wavelet_transform_cnn.py
python models/adamixnet.py
python models/compressor_vnvc.py
python models/ai_heads.py
```

### Integration Tests
```bash
# Test full pipeline
python -m pytest tests/ -v
```

## ğŸ“ˆ Monitoring

### TensorBoard
```bash
# Monitor training progress
tensorboard --logdir runs/
```

### Logging
- Training logs: `runs/stage{1,2,3}_{model}_{dataset}/`
- Model checkpoints: `checkpoints/`
- Evaluation results: `results/`
- Generated plots: `fig/`

## ğŸ›ï¸ Configuration

### Hyperparameters
- **Learning Rate**: 2e-4 vá»›i cosine decay
- **Batch Size**: 8 (default)
- **Seed**: 42 (for reproducibility)
- **Mixed Precision**: Enabled vá»›i AMP
- **Lambda Values**: [256, 512, 1024]

### Model Architecture
- **Wavelet Channels**: 64 (C')
- **AdaMixNet Output**: 128 (C_mix)
- **Latent Channels**: 192
- **Parallel Filters**: 4 (N)

## ğŸ“‹ Validation Checklist

Framework tá»± Ä‘á»™ng generate `checklist_report.md` Ä‘á»ƒ verify:

- âœ… WaveletTransformCNN cÃ³ Ä‘Ãºng PredictCNN & UpdateCNN layers
- âœ… AdaMixNet implement 4 parallel conv + softmax attention
- âœ… Compressor sá»­ dá»¥ng CompressAI GaussianConditional
- âœ… AI heads consume compressed features (khÃ´ng pixel reconstruction)
- âœ… Training scripts save checkpoints + TensorBoard logs
- âœ… Evaluation outputs CSV + RD-plots
- âœ… README cÃ³ dataset download + run commands

## ğŸ“Š Expected Results

### COCO 2017 Performance
| Lambda | PSNR (dB) | MS-SSIM | BPP | mAP@0.5 |
|--------|-----------|---------|-----|---------|
| 256    | ~28-30    | ~0.85   | 0.1 | ~0.65   |
| 512    | ~32-34    | ~0.90   | 0.2 | ~0.70   |
| 1024   | ~36-38    | ~0.95   | 0.4 | ~0.75   |

### DAVIS 2017 Performance
| Lambda | J&F Mean | Compression Ratio |
|--------|----------|-------------------|
| 256    | ~0.70    | 20:1              |
| 512    | ~0.75    | 10:1              |
| 1024   | ~0.80    | 5:1               |

## ğŸ”§ Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   ```bash
   # Reduce batch size
   --batch_size 4
   ```

2. **Dataset Not Found**
   ```bash
   # Re-run setup scripts
   bash datasets/setup_coco.sh
   bash datasets/setup_davis.sh
   ```

3. **Checkpoint Loading Error**
   ```bash
   # Check checkpoint path vÃ  compatibility
   python -c "import torch; print(torch.load('checkpoint.pth').keys())"
   ```

## ğŸ“š Citation

```bibtex
@article{wavenet_mv_2024,
  title={WAVENET-MV: Wavelet-based Video Compression for Machine Vision},
  author={Your Name},
  journal={arXiv preprint},
  year={2024}
}
```

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Make changes
4. Run tests
5. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file for details.

## ğŸ“ Support

- Issues: GitHub Issues
- Documentation: PROJECT_CONTEXT.md
- Results: RESULTS.md (auto-generated)

---

**Note**: Framework Ä‘Æ°á»£c design Ä‘á»ƒ cháº¡y trÃªn GPU. CPU training khÃ´ng Ä‘Æ°á»£c recommend do performance issues.

## CÃ i Ä‘áº·t

1. Clone repository:
```bash
git clone https://github.com/NgocMinhUET/WAVENET-MV.git
cd WAVENET-MV
```

2. CÃ i Ä‘áº·t dependencies:
```bash
pip install -r requirements.txt
```

3. CÃ i Ä‘áº·t dataset:
```bash
cd datasets
bash setup_coco.sh
```

## Training Pipeline

### CÃ¡ch 1: Sá»­ dá»¥ng script tá»± Ä‘á»™ng

Cháº¡y toÃ n bá»™ quÃ¡ trÃ¬nh training tá»± Ä‘á»™ng:

```bash
# TrÃªn Linux/Mac
bash server_training.sh

# TrÃªn Windows
# Sá»­ dá»¥ng Git Bash hoáº·c WSL Ä‘á»ƒ cháº¡y script bash
```

### CÃ¡ch 2: Training tá»«ng giai Ä‘oáº¡n

#### Stage 1: Train Wavelet Transform

```bash
python training/stage1_train_wavelet.py \
    --epochs 100 \
    --batch_size 16 \
    --learning_rate 1e-4 \
    --dataset coco \
    --data_dir datasets/COCO
```

#### Stage 2: Train Compressor

```bash
python training/stage2_train_compressor.py \
    --epochs 100 \
    --batch_size 8 \
    --learning_rate 1e-4 \
    --dataset coco \
    --data_dir datasets/COCO \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --lambda_rd 256
```

#### Stage 3: Train AI Heads

```bash
python training/stage3_train_ai.py \
    --epochs 50 \
    --batch_size 8 \
    --learning_rate 2e-4 \
    --dataset coco \
    --data_dir datasets/COCO \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda256_best.pth \
    --lambda_rd 256 \
    --enable_detection \
    --enable_segmentation
```

## ÄÃ¡nh giÃ¡

### ÄÃ¡nh giÃ¡ tá»± Ä‘á»™ng

Cháº¡y Ä‘Ã¡nh giÃ¡ Ä‘áº§y Ä‘á»§:

```bash
# TrÃªn Linux/Mac
bash server_evaluation.sh

# TrÃªn Windows
# Sá»­ dá»¥ng Git Bash hoáº·c WSL Ä‘á»ƒ cháº¡y script bash
```

### ÄÃ¡nh giÃ¡ thá»§ cÃ´ng

```bash
python evaluation/codec_metrics_final.py \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda256_best.pth \
    --dataset coco \
    --data_dir datasets/COCO \
    --split val \
    --max_samples 100 \
    --batch_size 4 \
    --output_file results/wavenet_mv_lambda256_evaluation.csv
```

## Táº¡o bÃ¡o cÃ¡o

Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p tá»« káº¿t quáº£ Ä‘Ã¡nh giÃ¡:

```bash
python evaluation/generate_summary_report.py \
    --input_dir results \
    --output_file results/wavenet_mv_comprehensive_results.csv
```

PhÃ¢n tÃ­ch thá»‘ng kÃª:

```bash
python evaluation/statistical_analysis.py \
    --input_file results/wavenet_mv_comprehensive_results.csv \
    --output_file results/wavelet_contribution_analysis.csv \
    --analysis_type wavelet \
    --visualize
```

## LÆ°u Ã½ quan trá»ng

1. **Fake Data**: Táº¥t cáº£ dá»¯ liá»‡u trong cÃ¡c file `.json` vÃ  `.csv` ban Ä‘áº§u lÃ  fake data, Ä‘Æ°á»£c táº¡o ra Ä‘á»ƒ minh há»a Ä‘á»‹nh dáº¡ng káº¿t quáº£. Äá»ƒ cÃ³ káº¿t quáº£ tháº­t, cáº§n cháº¡y training vÃ  evaluation.

2. **Checkpoints**: Cáº§n cháº¡y Ä‘áº§y Ä‘á»§ training pipeline Ä‘á»ƒ táº¡o ra cÃ¡c checkpoint tháº­t sá»± trÆ°á»›c khi Ä‘Ã¡nh giÃ¡.

3. **Lambda Values**: MÃ´ hÃ¬nh Ä‘Æ°á»£c train vá»›i nhiá»u giÃ¡ trá»‹ lambda khÃ¡c nhau (64, 128, 256, 512, 1024, 2048) Ä‘á»ƒ táº¡o ra cÃ¡c Ä‘iá»ƒm khÃ¡c nhau trÃªn Ä‘Æ°á»ng cong rate-distortion.

4. **Quantization**: ÄÃ£ sá»­a cÃ¡c bug liÃªn quan Ä‘áº¿n quantization collapse trong stage 2 training, Ä‘áº£m báº£o mÃ´ hÃ¬nh hoáº¡t Ä‘á»™ng Ä‘Ãºng.

## LiÃªn há»‡

Náº¿u cÃ³ báº¥t ká»³ cÃ¢u há»i hoáº·c gÃ³p Ã½ nÃ o, vui lÃ²ng liÃªn há»‡ qua email hoáº·c táº¡o issue trÃªn GitHub. 