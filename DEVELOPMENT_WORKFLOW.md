# ğŸ”„ WAVENET-MV Development Workflow
**Windows Development â†’ Ubuntu Server Training**

## ğŸ“‹ Quy TrÃ¬nh LÃ m Viá»‡c Hiá»‡u Quáº£

### ğŸªŸ **Phase 1: Development trÃªn Windows**

#### 1.1 Setup Ban Äáº§u (Chá»‰ lÃ m 1 láº§n)
```bash
# Cháº¡y setup
conda create -n wavenet-dev python=3.9
conda activate wavenet-dev
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install -r requirements.txt
pip install black flake8 isort pytest
```

#### 1.2 Quy TrÃ¬nh HÃ ng NgÃ y
```bash
# 1. KÃ­ch hoáº¡t environment
conda activate wavenet-dev

# 2. Sá»­ dá»¥ng Cursor AI Ä‘á»ƒ sá»­a code
# - Má»Ÿ Cursor, chá»‰nh sá»­a code
# - Sá»­ dá»¥ng AI Ä‘á»ƒ optimize, fix bugs

# 3. Test nhanh trÆ°á»›c khi push
python quick_test.py

# 4. Push code lÃªn git
git_workflow.bat "Describe your changes"
```

### ğŸ§ **Phase 2: Training trÃªn Ubuntu Server**

#### 2.1 Setup Server (Chá»‰ lÃ m 1 láº§n)
```bash
# TrÃªn Ubuntu server
chmod +x server_setup.sh
./server_setup.sh
```

#### 2.2 Quy TrÃ¬nh Training
```bash
# 1. Pull code má»›i nháº¥t
cd wavenet-mv
git pull origin master

# 2. Activate environment
source wavenet-prod/bin/activate

# 3. Cháº¡y training
python training/stage1_train_wavelet.py
python training/stage2_train_compressor.py
python training/stage3_train_ai.py

# 4. Monitor káº¿t quáº£
tensorboard --logdir=runs
```

## ğŸ¯ Workflow Tá»‘i Æ¯u

### ğŸ”„ **Chu TrÃ¬nh PhÃ¡t Triá»ƒn**
```
Windows (Development) â†’ Git â†’ Ubuntu (Training) â†’ Results â†’ Windows (Analysis)
```

### ğŸ“Š **PhÃ¢n Chia Nhiá»‡m Vá»¥**

| CÃ´ng Viá»‡c | NÆ¡i Thá»±c Hiá»‡n | LÃ½ Do |
|-----------|---------------|-------|
| Code development | Windows + Cursor AI | UI tá»‘t, AI assistant |
| Quick testing | Windows (CPU) | Kiá»ƒm tra syntax nhanh |
| Model training | Ubuntu Server (GPU) | TÃ i nguyÃªn tÃ­nh toÃ¡n |
| Result analysis | Windows | Visualization, charts |

### ğŸš€ **Commands Nhanh**

#### TrÃªn Windows:
```bash
# Test + Push trong 1 lá»‡nh
git_workflow.bat "Add new feature"

# Chá»‰ test
python quick_test.py

# Format code
black . && isort .
```

#### TrÃªn Ubuntu:
```bash
# Quick pull + train
git pull origin master && python training/stage1_train_wavelet.py

# Monitor training
htop  # System resources
nvidia-smi  # GPU usage
tensorboard --logdir=runs --port=6006
```

## ğŸ’¡ **Best Practices**

### âœ… **DOs**
- âœ… **LuÃ´n test trÆ°á»›c khi push**: `python quick_test.py`
- âœ… **Commit thÆ°á»ng xuyÃªn**: Nhá»¯ng thay Ä‘á»•i nhá», dá»… track
- âœ… **Sá»­ dá»¥ng meaningful commit messages**
- âœ… **Backup checkpoints**: Sao lÆ°u model weights quan trá»ng
- âœ… **Monitor training**: DÃ¹ng TensorBoard
- âœ… **Version control datasets**: Ghi láº¡i dataset version

### âŒ **DON'Ts**
- âŒ **KhÃ´ng push code chÆ°a test**
- âŒ **KhÃ´ng commit file lá»›n** (datasets, checkpoints)
- âŒ **KhÃ´ng hard-code paths**: DÃ¹ng relative paths
- âŒ **KhÃ´ng train trÃªn Windows**: Waste resources
- âŒ **KhÃ´ng ignore model weights**: ThÃªm vÃ o .gitignore

## ğŸ”§ **Troubleshooting**

### âš ï¸ **Lá»—i ThÆ°á»ng Gáº·p**

#### Windows:
```bash
# Lá»—i import
pip install -r requirements.txt

# Lá»—i torch
pip install torch --index-url https://download.pytorch.org/whl/cpu

# Lá»—i git
git config --global user.name "Your Name"
```

#### Ubuntu:
```bash
# Lá»—i CUDA
nvidia-smi  # Check GPU
pip install torch --index-url https://download.pytorch.org/whl/cu118

# Lá»—i memory
export CUDA_VISIBLE_DEVICES=0  # Limit GPU
```

## ğŸ“ˆ **Monitoring Training**

### ğŸ“Š **TensorBoard**
```bash
# TrÃªn server
tensorboard --logdir=runs --host=0.0.0.0 --port=6006

# TrÃªn Windows (SSH tunnel)
ssh -L 6006:localhost:6006 user@server-ip
# Má»Ÿ http://localhost:6006
```

### ğŸ“‹ **Log Files**
```bash
# Training logs
tail -f runs/stage1_*/events.out.tfevents.*

# System logs
tail -f /var/log/syslog
```

## ğŸ¯ **Káº¿t Quáº£ Mong Äá»£i**

Sau khi hoÃ n thÃ nh workflow nÃ y, báº¡n sáº½ cÃ³:
- âœ… **MÃ´i trÆ°á»ng development mÆ°á»£t mÃ ** trÃªn Windows
- âœ… **Quy trÃ¬nh test tá»± Ä‘á»™ng** trÆ°á»›c khi push
- âœ… **Server training á»•n Ä‘á»‹nh** trÃªn Ubuntu
- âœ… **Git workflow hiá»‡u quáº£**
- âœ… **Monitoring system** Ä‘áº§y Ä‘á»§

---
*ğŸ’¡ Tip: Bookmark file nÃ y vÃ  follow tá»«ng bÆ°á»›c cho láº§n Ä‘áº§u setup!* 

## ğŸ”„ **GIT WORKFLOW: DEVELOPMENT (MÃ¡y A) â†’ SERVER (MÃ¡y B)**

### **Quy trÃ¬nh tá»•ng quan**
```
MÃ¡y A (Windows - Development) â†’ Git Repository â†’ MÃ¡y B (Server - Training)
```

### **BÆ°á»›c 1: PhÃ¡t triá»ƒn trÃªn MÃ¡y A (Windows)**

#### 1.1 Kiá»ƒm tra vÃ  commit thay Ä‘á»•i
```bash
# Kiá»ƒm tra status
git status

# ThÃªm file Ä‘Ã£ sá»­a
git add .

# Commit vá»›i message rÃµ rÃ ng
git commit -m "Fix: [mÃ´ táº£ lá»—i Ä‘Æ°á»£c sá»­a]"
# hoáº·c
git commit -m "Feature: [tÃ­nh nÄƒng má»›i]"
# hoáº·c
git commit -m "Update: [cáº­p nháº­t nÃ o]"
```

#### 1.2 Push lÃªn remote repository
```bash
git push origin master
```

### **BÆ°á»›c 2: Cháº¡y trÃªn MÃ¡y B (Server)**

#### 2.1 Clone repository (láº§n Ä‘áº§u)
```bash
git clone [YOUR_REPO_URL]
cd wavenet-mv
```

#### 2.2 Pull latest changes (cÃ¡c láº§n sau)
```bash
git pull origin master
```

#### 2.3 Setup environment (náº¿u cáº§n)
```bash
# Install dependencies
pip install -r requirements.txt

# Setup datasets
./datasets/setup_coco.sh
./datasets/setup_davis.sh
```

#### 2.4 Cháº¡y training
```bash
# Stage 1: Wavelet Training
python training/stage1_train_wavelet.py

# Stage 2: Compressor Training  
python training/stage2_train_compressor.py

# Stage 3: AI Heads Training
python training/stage3_train_ai.py
```

### **BÆ°á»›c 3: Xá»­ lÃ½ lá»—i (Error Handling)**

#### 3.1 Khi gáº·p lá»—i trÃªn MÃ¡y B
1. **Copy full error message** (bao gá»“m traceback)
2. **Copy command Ä‘Ã£ cháº¡y**
3. **Note mÃ´i trÆ°á»ng** (Python version, CUDA, etc)

#### 3.2 Debug trÃªn MÃ¡y A
1. Paste error vÃ o Cursor AI
2. PhÃ¢n tÃ­ch vÃ  sá»­a lá»—i
3. Test local náº¿u cÃ³ thá»ƒ
4. Commit fix:
```bash
git add .
git commit -m "Fix: [mÃ´ táº£ lá»—i cá»¥ thá»ƒ]"
git push origin master
```

#### 3.3 Quay láº¡i MÃ¡y B
```bash
git pull origin master
# Cháº¡y láº¡i command bá»‹ lá»—i
```

### **ğŸ“‹ Checklist trÆ°á»›c khi Push**

**TrÃªn MÃ¡y A:**
- [ ] Code khÃ´ng cÃ³ syntax error
- [ ] Commit message rÃµ rÃ ng
- [ ] ÄÃ£ test cÆ¡ báº£n (náº¿u cÃ³ thá»ƒ)
- [ ] Cáº­p nháº­t requirements.txt náº¿u thÃªm dependency

**TrÃªn MÃ¡y B:**
- [ ] Pull latest changes
- [ ] Check Python environment
- [ ] Verify dataset paths
- [ ] Check disk space cho checkpoints

### **ğŸ”§ Useful Git Commands**

```bash
# Kiá»ƒm tra commit history
git log --oneline -10

# So sÃ¡nh vá»›i remote
git fetch
git diff HEAD origin/master

# Rollback náº¿u cáº§n
git reset --hard HEAD~1

# Táº¡o branch cho feature lá»›n
git checkout -b feature/new-architecture
git push -u origin feature/new-architecture
```

### **ğŸ“Š Monitoring Commands cho MÃ¡y B**

```bash
# Check GPU usage
nvidia-smi

# Monitor training progress
tensorboard --logdir=./runs

# Check disk space
df -h

# Check running processes
ps aux | grep python
```

### **ğŸš¨ Common Issues & Solutions**

| Issue | Solution |
|-------|----------|
| `git pull` conflicts | `git stash` â†’ `git pull` â†’ `git stash pop` |
| CUDA out of memory | Reduce batch size in config |
| Dataset not found | Check paths in `dataset_loaders.py` |
| Permission denied | `chmod +x scripts/*.sh` |
| Python version mismatch | Use conda/venv with exact version |

### **ğŸ“ Error Reporting Template**

```
**Environment:**
- OS: [Ubuntu 20.04 / CentOS 7 / etc]
- Python: [3.8.x]
- PyTorch: [1.13.x]
- CUDA: [11.6]

**Command:**
```bash
[exact command that failed]
```

**Error:**
```
[full traceback]
```

**Additional Info:**
- Commit hash: [git rev-parse HEAD]
- Disk space: [df -h]
- GPU info: [nvidia-smi]
```

## ğŸš€ **COMPLETE 3-STAGE TRAINING PIPELINE**

### **ğŸ¯ ToÃ n Cáº£nh Workflow**
```
Stage 1: WaveletCNN (30 epochs) â†’ Stage 2: Compressor (40 epochs) â†’ Stage 3: AI Heads (50 epochs)
```

### **ğŸ“‹ Stage-by-Stage Commands**

#### **Stage 1: Wavelet Training** âœ…
```bash
python training/stage1_train_wavelet.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --epochs 30 \
    --batch_size 8
```

#### **Stage 2: Compressor Training** âœ… (FIXED BUGS)
```bash
python training/stage2_train_compressor.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --lambda_rd 128 \
    --epochs 40 \
    --batch_size 8
```

#### **ğŸ†• Stage 3: AI Heads Training** 
```bash
# Detection Only
python training/stage3_train_ai.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda128_best.pth \
    --lambda_rd 128 \
    --enable_detection \
    --epochs 50 \
    --batch_size 4

# Segmentation Only  
python training/stage3_train_ai.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda128_best.pth \
    --lambda_rd 128 \
    --enable_segmentation \
    --epochs 50 \
    --batch_size 4

# Both Tasks
python training/stage3_train_ai.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda128_best.pth \
    --lambda_rd 128 \
    --enable_detection \
    --enable_segmentation \
    --epochs 50 \
    --batch_size 4
```

---

## ğŸ¯ **ROADMAP SAU KHI HOÃ€N THÃ€NH STAGE 2**

### **BÆ°á»›c 1: Kiá»ƒm Tra Stage 2 Results** 
```bash
# Kiá»ƒm tra checkpoints
ls -la checkpoints/stage2_*

# Kiá»ƒm tra TensorBoard logs
tensorboard --logdir runs/stage2_*

# Expected files:
# - checkpoints/stage2_compressor_coco_lambda128_best.pth
# - checkpoints/stage2_compressor_coco_lambda128_latest.pth
```

### **BÆ°á»›c 2: Push Stage 2 Results (Windows)**
```bash
git add .
git commit -m "Complete Stage 2: Compressor training with MSE fixes
- MSE stable at 0.001-0.1 range (not collapsing)
- BPP in 1-10 range (proper calculation)  
- Ready for Stage 3 AI heads training"
git push origin master
```

### **BÆ°á»›c 3: Start Stage 3 Training (Server)**
```bash
# Pull latest changes
git pull origin master

# Start with detection task (easier to debug)
python training/stage3_train_ai.py \
    --dataset coco \
    --data_dir datasets/COCO_Official \
    --stage1_checkpoint checkpoints/stage1_wavelet_coco_best.pth \
    --stage2_checkpoint checkpoints/stage2_compressor_coco_lambda128_best.pth \
    --lambda_rd 128 \
    --enable_detection \
    --epochs 50 \
    --batch_size 4 \
    --learning_rate 1e-4
```

### **BÆ°á»›c 4: Monitor Stage 3 Training**
```bash
# TensorBoard monitoring
tensorboard --logdir runs/stage3_ai_heads_*

# Expected logs:
# - Train/TotalLoss
# - Train/DetectionLoss  
# - Train/SegmentationLoss
# - Val/TotalLoss
```

### **BÆ°á»›c 5: Evaluation & Comparison**
```bash
# Sau khi Stage 3 hoÃ n thÃ nh
python evaluation/codec_metrics.py \
    --model_path checkpoints/stage3_ai_heads_coco_best.pth \
    --dataset coco \
    --output_dir results/

# Compare vá»›i baselines
python evaluation/compare_baselines.py \
    --wavenet_path checkpoints/stage3_ai_heads_coco_best.pth \
    --output_csv results/comparison.csv
```

---

## ğŸ“Š **FULL PIPELINE ARCHITECTURE**

```
Input Image (3Ã—256Ã—256)
    â†“
Stage 1: WaveletCNN â†’ Wavelet Coeffs (4Ã—64Ã—HÃ—W) 
    â†“  
Stage 2a: AdaMixNet â†’ Mixed Features (128Ã—H/4Ã—W/4)
    â†“
Stage 2b: CompressorVNVC â†’ Compressed Features (128Ã—H/4Ã—W/4) + BPP
    â†“
Stage 3a: YOLO-tiny â†’ Detection Boxes [x,y,w,h,conf,class]
Stage 3b: SegFormer â†’ Segmentation Masks (21Ã—HÃ—W)
```

---

## ğŸ‰ **SUCCESS CRITERIA**

### **Stage 2 Completion Indicators:**
- âœ… MSE: 0.001-0.1 (stable, khÃ´ng collapse)
- âœ… BPP: 1-10 (reasonable compression rate)
- âœ… Health check: "âœ… MSE HEALTHY" + "âœ… BALANCED"
- âœ… Debug: "âœ… CompressorVNVC applying compression"

### **Stage 3 Success Indicators:**
- âœ… Detection Loss: Decreasing smoothly
- âœ… Segmentation Loss: Converging  
- âœ… Frozen pipeline: Compression features remain consistent
- âœ… GPU memory: Efficient usage vá»›i batch_size=4

### **Final Pipeline Success:**
- âœ… Full WAVENET-MV working end-to-end
- âœ… Competitive vá»›i JPEG/H.264 baselines
- âœ… Real-time inference capable
- âœ… Multiple task performance

--- 