# COCO DATASET SETUP GUIDE - WAVENET-MV

## ğŸ¯ **OFFICIAL COCO DATASET INTEGRATION**

Dá»±a trÃªn [COCO official website](https://cocodataset.org/#download) vÃ  [Ultralytics documentation](https://docs.ultralytics.com/datasets/detect/coco/), guide nÃ y sáº½ giÃºp báº¡n download vÃ  setup COCO dataset chÃ­nh thá»©c cho WAVENET-MV framework.

---

## ğŸ“Š **COCO DATASET OVERVIEW**

### **Dataset Statistics**
- **Total Images**: 330K images
- **Annotated Images**: 200K images vá»›i detailed annotations
- **Object Categories**: 80 classes (person, car, bicycle, etc.)
- **Tasks**: Detection, Segmentation, Keypoints, Captioning

### **Dataset Splits**
| Split | Images | Size | Usage |
|-------|--------|------|-------|
| **Train2017** | 118K | ~19GB | Training models |
| **Val2017** | 5K | ~1GB | Validation during training |
| **Test2017** | 20K | ~7GB | Final evaluation |
| **Annotations** | - | ~241MB | Object detection annotations |

---

## ğŸš€ **QUICK SETUP (RECOMMENDED)**

### **Option 1: Minimal Setup (Testing)**
```bash
# Download validation set + annotations (~1.2GB)
python datasets/setup_coco_official.py --minimal

# Result: datasets/COCO/ vá»›i val2017 images + annotations
```

### **Option 2: Full Setup (Production)**  
```bash
# Download complete dataset (~46GB)
python datasets/setup_coco_official.py --full

# Result: Complete COCO 2017 dataset
```

### **Option 3: Custom Setup**
```bash
# Download specific subsets
python datasets/setup_coco_official.py --datasets val2017 train2017 annotations_trainval2017

# Custom directory
python datasets/setup_coco_official.py --minimal --dir /path/to/my/coco
```

---

## ğŸ“ **EXPECTED DIRECTORY STRUCTURE**

Sau khi setup thÃ nh cÃ´ng, báº¡n sáº½ cÃ³ cáº¥u trÃºc:

```
datasets/COCO/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train2017/        # 118,287 training images
â”‚   â”‚   â”œâ”€â”€ 000000000009.jpg
â”‚   â”‚   â”œâ”€â”€ 000000000025.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ val2017/          # 5,000 validation images  
â”‚   â”‚   â”œâ”€â”€ 000000000139.jpg
â”‚   â”‚   â”œâ”€â”€ 000000000285.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ test2017/         # 40,670 test images (optional)
â”‚       â””â”€â”€ ...
â”œâ”€â”€ annotations/
â”‚   â”œâ”€â”€ instances_train2017.json    # Training annotations
â”‚   â”œâ”€â”€ instances_val2017.json      # Validation annotations
â”‚   â”œâ”€â”€ person_keypoints_train2017.json
â”‚   â”œâ”€â”€ person_keypoints_val2017.json
â”‚   â”œâ”€â”€ captions_train2017.json
â”‚   â””â”€â”€ captions_val2017.json
â”œâ”€â”€ labels/               # YOLO format (optional)
â”‚   â”œâ”€â”€ train2017/
â”‚   â””â”€â”€ val2017/
â””â”€â”€ dataset_info.json     # Setup information
```

---

## ğŸ”§ **WAVENET-MV INTEGRATION**

### **COCODatasetLoader Compatibility**
COCO dataset structure hoÃ n toÃ n compatible vá»›i COCODatasetLoader:

```python
from datasets.dataset_loaders import COCODatasetLoader

# Load validation set
dataset = COCODatasetLoader(
    data_dir='datasets/COCO',  # Path to downloaded COCO
    subset='val',              # Use val2017
    image_size=256,            # Resize for WAVENET-MV
    task='detection',          # Object detection
    augmentation=True          # Data augmentation
)

print(f"Dataset length: {len(dataset)}")  # Should show 5,000 for val2017
sample = dataset[0]
print(f"Sample keys: {list(sample.keys())}")  # ['image', 'boxes', 'labels']
```

### **Training Integration**
```python
# Stage 1: Wavelet Training
# Uses COCO images for reconstruction loss
python training/stage1_train_wavelet.py

# Stage 2: Compression Training  
# Uses COCO images for rate-distortion optimization
python training/stage2_train_compressor.py

# Stage 3: AI Heads Training
# Uses COCO images + annotations for detection/segmentation
python training/stage3_train_ai.py
```

---

## ğŸŒ **DOWNLOAD METHODS**

### **Method 1: Python Script (Recommended)**
```bash
# Use official setup script
python datasets/setup_coco_official.py --minimal
```

### **Method 2: Manual Download**
Theo [COCO website](https://cocodataset.org/#download):

```bash
# Create directories
mkdir -p datasets/COCO/images
cd datasets/COCO

# Download images
wget http://images.cocodataset.org/zips/val2017.zip     # 1GB
wget http://images.cocodataset.org/zips/train2017.zip   # 19GB (optional)

# Download annotations
wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip  # 241MB

# Extract
unzip val2017.zip -d images/
unzip annotations_trainval2017.zip
```

### **Method 3: Using Ultralytics**
```python
from ultralytics.utils.downloads import download

# Download using Ultralytics helper
urls = [
    "http://images.cocodataset.org/zips/val2017.zip",
    "http://images.cocodataset.org/annotations/annotations_trainval2017.zip"
]
download(urls, dir="datasets/COCO", threads=2)
```

---

## âœ… **VERIFICATION CHECKLIST**

### **After Download, Verify:**
```bash
# Verify dataset structure
python datasets/setup_coco_official.py --verify-only --dir datasets/COCO
```

**Expected Output:**
```
ğŸ” Verifying COCO dataset structure...
âœ… images/val2017 (5,000 images)
âœ… annotations/instances_val2017.json (25MB)
âœ… annotations/instances_train2017.json (123MB) 
ğŸ‰ COCO dataset structure verified successfully!
```

### **Test DataLoader:**
```bash
# Test COCO loading
python -c "
from datasets.dataset_loaders import COCODatasetLoader
dataset = COCODatasetLoader(data_dir='datasets/COCO', subset='val')
print(f'âœ… Loaded {len(dataset)} samples')
sample = dataset[0]
print(f'âœ… Sample shape: {sample[\"image\"].shape}')
print(f'âœ… Boxes shape: {sample[\"boxes\"].shape}')
print(f'âœ… Labels shape: {sample[\"labels\"].shape}')
"
```

---

## ğŸ¨ **COCO CATEGORIES (80 Classes)**

COCO dataset bao gá»“m 80 object categories:

```python
COCO_CLASSES = [
    'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat',
    'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat',
    'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack',
    'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',
    'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
    'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',
    'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop',
    'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink',
    'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
]
```

---

## âš¡ **PERFORMANCE TIPS**

### **Fast Download:**
```bash
# Parallel downloads (náº¿u cÃ³ bandwidth)
# Method from community: https://gist.github.com/mkocabas/a6177fc00315403d31572e17700d7fd9
wget -c http://images.cocodataset.org/zips/val2017.zip &
wget -c http://images.cocodataset.org/annotations/annotations_trainval2017.zip &
wait
```

### **Space Optimization:**
```bash
# Download + extract + cleanup in one go
python datasets/setup_coco_official.py --minimal  # Auto cleanup ZIP files
```

### **Resume Downloads:**
```bash
# If download interrupted
python datasets/setup_coco_official.py --minimal --force  # Re-download
```

---

## ğŸ”§ **TROUBLESHOOTING**

### **Common Issues:**

#### **Issue 1: Slow Download**
```bash
# Solution: Use parallel downloads hoáº·c different mirror
# Try: aria2c for faster downloads
aria2c -x 4 http://images.cocodataset.org/zips/val2017.zip
```

#### **Issue 2: Space Issues**
```bash
# Check available space
df -h .
# Minimal setup chá»‰ cáº§n ~1.5GB free space
python datasets/setup_coco_official.py --minimal
```

#### **Issue 3: Network Issues**
```bash
# Resume interrupted downloads
python datasets/setup_coco_official.py --minimal --force
```

#### **Issue 4: Permission Issues**
```bash
# Fix permissions
chmod -R 755 datasets/COCO/
```

---

## ğŸ“Š **TRAINING RECOMMENDATIONS**

### **For WAVENET-MV Training:**

1. **Development/Testing**: Use `--minimal` (val2017 only)
   - Fast download (~1.2GB)
   - Good for debugging vÃ  initial testing
   - 5,000 validation images

2. **Full Training**: Use `--full` (train+val+test)
   - Complete dataset (~46GB)  
   - Production-ready training
   - 118,000+ training images

3. **Custom Training**: Mix vÃ  match
   - Training: Use train2017 (118K images)
   - Validation: Use val2017 (5K images)
   - Testing: Use test2017 (40K images)

### **WAVENET-MV Stages:**
- **Stage 1** (Wavelet): Images only (no annotations needed)
- **Stage 2** (Compression): Images only (optional annotations)  
- **Stage 3** (AI Heads): Images + annotations required

---

## ğŸ“š **REFERENCES**

- **COCO Official**: https://cocodataset.org/#download
- **COCO Paper**: [Microsoft COCO: Common Objects in Context](https://arxiv.org/abs/1405.0312)
- **Ultralytics COCO**: https://docs.ultralytics.com/datasets/detect/coco/
- **Community Scripts**: https://gist.github.com/mkocabas/a6177fc00315403d31572e17700d7fd9

---

## ğŸ¯ **NEXT STEPS**

1. **Download Dataset**: `python datasets/setup_coco_official.py --minimal`
2. **Verify Setup**: `python datasets/setup_coco_official.py --verify-only`
3. **Test Loading**: Test COCODatasetLoader compatibility
4. **Start Training**: Begin Stage 1 WAVENET-MV training
5. **Scale Up**: Download full dataset when ready for production

**Ready to revolutionize video compression vá»›i WAVENET-MV + COCO! ğŸš€** 